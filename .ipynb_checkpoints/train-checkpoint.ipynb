{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "\n",
    "## Unet (tf 2)  Train on Custom data (Data-set Taken from the Kaggle)\n",
    "\n",
    "#### Written by Sohaib Anwaar (Data-scientist)    \n",
    "\n",
    "    For training unet on your data you first need to annotate images. I annotate images with the help of VIA Image annotator I select polygons and sorround the objects with polygon you guys can easily find VIA image annotator on google.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import unet\n",
    "from unet import utils\n",
    "import glob\n",
    "from unet.datasets import circles\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data from Json to the Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Description:\n",
      "        This Generator will help you out in converting your annotations from polygon to mask image\n",
      "        so that you can use it for the training. I done annotaions with VIA Image annotator with\n",
      "        polygons and make this fucntion to convert those annotations to the mask so that I can use\n",
      "        it to train my unet model\n",
      "        \n",
      "    Input:\n",
      "        json_path  (str) : Path of the json in which you saved your annnotations\n",
      "        base_path  (str) : Base path where your images are located\n",
      "        save_masks (str) : Location where you want to save the masks\n",
      "        \n",
      "    Output:\n",
      "        Image      (nd_array) : Numpy array of original Image\n",
      "        Mask       (nd_array) : Numpy array of mask image\n",
      "    \n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from unet.dataset_utils import convert_json_to_mask\n",
    "import matplotlib.pyplot as plt    \n",
    "    \n",
    "    \n",
    "base_path  = \"/media/sohaib/additional_/DataScience/unet/image/\"  \n",
    "save_masks = \"/media/sohaib/additional_/DataScience/unet/mask/\"\n",
    "json_path  = f\"{base_path}via_region_data.json\"\n",
    "\n",
    "print(convert_json_to_mask.__doc__)\n",
    "con = convert_json_to_mask(json_path, base_path, save_masks)\n",
    "for original_img, label in con:\n",
    "    if original_img is not None:\n",
    "        plt.imshow(original_img)\n",
    "        plt.show()\n",
    "        plt.imshow(label)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path          = \"/media/sohaib/additional_/DataScience/unet/\"    \n",
    "augmentation_training = 2\n",
    "\n",
    "images_count = len(glob.glob(f\"{dataset_path}/image/*.*g\"))\n",
    "\n",
    "train_dataset, validation_dataset = circles.load_data(images_count, dataset_path, augmentation_training , splits=(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 224, 224\n",
    "batch_size = 6\n",
    "processed_train_dataset      = train_dataset.map(utils.expand_dims_([w, h])).repeat().batch(batch_size)\n",
    "processed_validation_dataset = validation_dataset.map(utils.expand_dims_([w, h])).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "dataset = train_dataset.batch(batch_size=4)\n",
    "\n",
    "rows, cols = 3, 2\n",
    "count      = 0\n",
    "fig, ax = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(10,10))\n",
    "\n",
    "for index, i in enumerate(dataset):\n",
    "    \n",
    "    # Converting Tensorflow to Numpy\n",
    "    images, labels = i\n",
    "    labels = labels.numpy()\n",
    "    images = images.numpy()\n",
    "    \n",
    "    print(labels.shape)\n",
    "    \n",
    "    # Displaying Training and testing Images\n",
    "    ax[count][0].matshow(images[1]); ax[count][0].set_title('Original Image'); ax[count][0].axis('off')\n",
    "    ax[count][1].matshow(labels[1]); ax[count][1].set_title('Original Mask'); ax[count][1].axis('off')\n",
    "    \n",
    "    count +=1\n",
    "    if count >= ((rows * cols)/2): break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in processed_validation_dataset.take(1):\n",
    "    sample_image, sample_mask = image[0], mask[0]\n",
    "    print(sample_image.shape)\n",
    "    \n",
    "\n",
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(sample_image, 0))\n",
    "print (down_result.shape)\n",
    "\n",
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Generator(OUTPUT_CHANNELS):\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 1])\n",
    "\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
    "    downsample(128, 4),  # (bs, 64, 64, 128)\n",
    "    downsample(256, 4),  # (bs, 32, 32, 256)\n",
    "    downsample(512, 4),  # (bs, 16, 16, 512)\n",
    "    downsample(512, 4),  # (bs, 8, 8, 512)\n",
    "    downsample(512, 4),  # (bs, 4, 4, 512)\n",
    "    downsample(512, 4),  # (bs, 2, 2, 512)\n",
    "    downsample(512, 4),  # (bs, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
    "    upsample(512, 4),  # (bs, 16, 16, 1024)\n",
    "    upsample(256, 4),  # (bs, 32, 32, 512)\n",
    "    upsample(128, 4),  # (bs, 64, 64, 256)\n",
    "    upsample(64, 4),  # (bs, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='sigmoid')  # (bs, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1\n",
    "generator = Generator(OUTPUT_CHANNELS)\n",
    "\n",
    "generator.compile(optimizer=tf.keras.optimizers.Adam(0.0001,  clipnorm=1.0),\n",
    "              loss=[dice_loss],\n",
    "              metrics=[dice_coe])\n",
    "generator.load_weights(\"/media/sohaib/additional_/DataScience/unet/weights/Unet_0.8349606990814209_17_.h5\")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "for image, mask in processed_validation_dataset.take(1):\n",
    "    sample_image, sample_mask = image[0], mask[0]\n",
    "    print(sample_mask.shape)\n",
    "    \n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        display([sample_image, sample_mask,\n",
    "                 create_mask(generator.predict(sample_image[tf.newaxis, ...]))])\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "save_weights_here = \"/media/sohaib/additional_/DataScience/unet/weights/\"\n",
    "weights_name = \"Unet_{val_dice_coe}_{epoch}_.h5\"\n",
    "file_path = f\"{save_weights_here}{weights_name}\"\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    file_path, monitor='val_loss', verbose=0, save_best_only=False,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "\n",
    "model_history = generator.fit(processed_train_dataset, epochs=EPOCHS,\n",
    "                          steps_per_epoch=600,\n",
    "                          validation_steps=300,\n",
    "                          validation_data=processed_validation_dataset,\n",
    "                          callbacks=[DisplayCallback(), checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = generator.predict(validation_dataset.batch(batch_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(prediction[i,...], axis=-1)\n",
    "plt.imshow(np.argmax(prediction[5,...], axis=-1),  cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(10,10))\n",
    "dataset = validation_dataset.map(utils.crop_image_and_label_to_shape(prediction.shape[1:]))\n",
    "\n",
    "for i, (image, label) in enumerate(dataset.take(3)):\n",
    "    ax[i][0].matshow(image[..., -1]); ax[i][0].set_title('Original Image'); ax[i][0].axis('off')\n",
    "    ax[i][1].matshow(np.argmax(label, axis=-1), cmap=plt.cm.gray); ax[i][1].set_title('Original Mask'); ax[i][1].axis('off')\n",
    "    ax[i][2].matshow(np.argmax(prediction[i,...], axis=-1), cmap=plt.cm.gray); ax[i][2].set_title('Predicted Mask'); ax[i][2].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet.datasets.circles import _create_image_and_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "dataset_path  = \"/media/sohaib/additional_/DataScience/unet/\"    \n",
    "training_images, training_masks = _create_image_and_mask(dataset_path, augmentation_rotation = 90)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "rows, cols = 3, 2\n",
    "fig, ax = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(10,10))\n",
    "for i, (image, label) in enumerate(zip(training_images, training_masks)):\n",
    "    \n",
    "    if (i * 2) != (rows * cols):\n",
    "        print(image.shape)\n",
    "        print(label.shape)\n",
    "        ax[i][0].matshow(image); ax[i][0].set_title('Original Image'); ax[i][0].axis('off')\n",
    "        ax[i][1].matshow(np.argmax(label, axis=-1), cmap=plt.cm.gray); ax[i][1].set_title('Original Mask'); ax[i][1].axis('off')\n",
    "    else:break\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
